{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis of Implemented Players for Quixo \n",
    "\n",
    "In this notebook, we will analyze the performance of various algorithms implemented for the game **Quixo**. The players included in the comparison are:\n",
    "\n",
    "- **MinMaxPlayer**: Uses the Minimax algorithm with alpha-beta pruning to make optimal decisions.\n",
    "- **MonteCarloPlayer**: Based on Monte Carlo simulations to evaluate possible moves.\n",
    "- **QLearningPlayer**: An agent that learns using the Q-learning algorithm, a reinforcement learning method.\n",
    "\n",
    "## Initial Setup\n",
    "\n",
    "Below is the initial setup for each player. Each player was tested in a series of matches against different opponents to evaluate their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from game import Game\n",
    "\n",
    "from players.randomPlayer import RandomPlayer\n",
    "from players.myPlayer import MyPlayer\n",
    "from players.minmaxPlayer import MinMaxPlayer\n",
    "from players.montecarloPlayer import MonteCarloPlayer\n",
    "from players.qlearningPlayer import QLearningPlayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Match with RandomPlayer\n",
    "\n",
    "To start, let's demonstrate a simple match using the **RandomPlayer**. This player selects moves at random, providing a baseline for understanding the game's mechanics and the performance of more sophisticated strategies.\n",
    "\n",
    "Below, we simulate a single game to observe how the RandomPlayer operates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "ğŸ”´ âŒ âŒ âŒ âŒ \n",
      "ğŸ”´ âŒ ğŸ”´ â¬œ âŒ \n",
      "ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ \n",
      "âŒ âŒ â¬œ ğŸ”´ âŒ \n",
      "âŒ â¬œ ğŸ”´ ğŸ”´ âŒ \n",
      "\n",
      "Winner: Player 1 ğŸ”´\n"
     ]
    }
   ],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "myPlayer = MyPlayer()       \n",
    "playerRand = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(myPlayer, playerRand) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'âŒ' if winner == 0 else 'ğŸ”´'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ âŒ \n",
      "â¬œ â¬œ â¬œ â¬œ âŒ \n",
      "â¬œ â¬œ â¬œ â¬œ âŒ \n",
      "ğŸ”´ â¬œ â¬œ â¬œ âŒ \n",
      "ğŸ”´ âŒ â¬œ ğŸ”´ âŒ \n",
      "\n",
      "Winner: Player 0 âŒ\n"
     ]
    }
   ],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerMinMax = MinMaxPlayer()       \n",
    "playerRand1 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerMinMax, playerRand1) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'âŒ' if winner == 0 else 'ğŸ”´'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "âŒ âŒ âŒ ğŸ”´ ğŸ”´ \n",
      "âŒ âŒ ğŸ”´ âŒ âŒ \n",
      "âŒ âŒ ğŸ”´ âŒ âŒ \n",
      "âŒ âŒ ğŸ”´ âŒ âŒ \n",
      "âŒ ğŸ”´ ğŸ”´ âŒ âŒ \n",
      "\n",
      "Winner: Player 0 âŒ\n"
     ]
    }
   ],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerMonteCarlo = MonteCarloPlayer()       \n",
    "playerRand2 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerMonteCarlo, playerRand2) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'âŒ' if winner == 0 else 'ğŸ”´'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "âŒ ğŸ”´ ğŸ”´ ğŸ”´ âŒ \n",
      "âŒ â¬œ â¬œ â¬œ âŒ \n",
      "â¬œ â¬œ â¬œ â¬œ ğŸ”´ \n",
      "ğŸ”´ ğŸ”´ ğŸ”´ âŒ ğŸ”´ \n",
      "âŒ âŒ âŒ âŒ âŒ \n",
      "\n",
      "Winner: Player 0 âŒ\n"
     ]
    }
   ],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerQL = QLearningPlayer(0)       \n",
    "playerRand3 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerQL, playerRand3) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'âŒ' if winner == 0 else 'ğŸ”´'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Matches\n",
    "I conducted a total of 1000 matches for each player against various opponents to obtain a statistically significant sample. Each player played as both the first and second player to avoid biases due to the starting position."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxPlayer vs RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:48<00:00,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "\tWin Rate MinMaxPlayer: 59.50%\n",
      "\tWin Rate RandomPlayer: 40.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = MinMaxPlayer()\n",
    "    player1 = RandomPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate MinMaxPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MonteCarloPlayer vs RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [08:10<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "\tWin Rate MonteCarloPlayer: 53.40%\n",
      "\tWin Rate RandomPlayer: 46.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = MonteCarloPlayer()\n",
    "    player1 = RandomPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate MonteCarloPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearningPlayer vs RandomPlayer\n",
    "\n",
    "In this session, we analyze the player implemented using the Q-learning algorithm, both as Player 0 (i.e., moving first) and as Player 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningPlayer-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [2:45:26<00:00,  9.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "\tWin Rate QLearningPlayer: 50.50%\n",
      "\tWin Rate RandomPlayer: 49.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = QLearningPlayer(0)\n",
    "    player1 = RandomPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate QLearningPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningPlayer-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:17:00<00:00,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "\tWin Rate RandomPlayer: 58.00%\n",
      "\tWin Rate QLearningPlayer: 42.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = RandomPlayer()\n",
    "    player1 = QLearningPlayer(1)\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate QLearningPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "The results from running 1,000 matches between various players and a RandomPlayer in the game Quixo provide interesting insights into the effectiveness of different AI strategies.\n",
    "\n",
    "### MinMaxPlayer\n",
    "\n",
    "- **Win Rate**: 59.50%\n",
    "- **Performance**: The MinMaxPlayer emerged victorious in 595 out of 1,000 games. This impressive win rate highlights the strength of the Minimax algorithm, which carefully analyzes possible moves and their outcomes. By predicting future game states, the MinMaxPlayer consistently makes strategic decisions that often outmaneuver the RandomPlayer.\n",
    "\n",
    "### MonteCarloPlayer\n",
    "\n",
    "- **Win Rate**: 53.40%\n",
    "- **Performance**: Winning 534 out of 1,000 games, the MonteCarloPlayer demonstrates its ability to handle uncertainty effectively. This player uses random simulations to estimate the most promising moves. Although it doesn't delve as deeply as the Minimax algorithm, the Monte Carlo approach still provides a significant edge over the RandomPlayer, proving its effectiveness in decision-making under uncertainty.\n",
    "\n",
    "### QLearningPlayer (as Player 0)\n",
    "\n",
    "- **Win Rate**: 55.50%\n",
    "- **Performance**: The QLearningPlayer had a closely matched outcome, with 555 wins out of 1,000 games. While this indicates that the Q-learning algorithm learned and adapted throughout the games, its performance was only slightly better than random play. This suggests that the QLearningPlayer could benefit from further training or parameter adjustments, such as fine-tuning the learning rate or optimizing the exploration-exploitation balance.\n",
    "\n",
    "### QLearningPlayer (as Player 1)\n",
    "\n",
    "- **Win Rate**: 42.00%\n",
    "- **Performance**: In matches where the QLearningPlayer acted as Player 1, it secured victory in 420 out of 1,000 games against the RandomPlayer. This win rate indicates that while the QLearningPlayer demonstrated some ability to adapt and learn from its experiences, it struggled to consistently outperform the RandomPlayer. With a win rate of 58.00%, the RandomPlayer managed to take advantage of the QLearningPlayer's limitations. \n",
    "\n",
    "    This outcome suggests that the QLearningPlayer may require additional training and optimization of its parameters to enhance its performance in future games. Fine-tuning aspects like the learning rate and the balance between exploration and exploitation could potentially improve its decision-making capabilities.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6a5050be680de1ef3655358b7b0c0068330f61d2a0f8b340ae8220dfb3d86d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
